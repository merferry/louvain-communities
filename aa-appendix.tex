\subsection{Indirect Comparison with State-of-the-art Louvain Implementations}
\label{sec:comparison-indirect}

We now indirectly compare the performance of our multicore implementation of Louvain algorithm with other similar state-of-the-art implementations, listed in Table \ref{tab:compare}. Mohammadi et al. \cite{com-mohammadi20} introduce the Adaptive CUDA Louvain Method (ACLM), employing GPU acceleration. Their approach involves evaluating the change in modularity for each edge in the graph, reflecting the impact of moving the source vertex to the community of the target vertex. Notably, only the computation of modularity changes occurs on the GPU, with other algorithmic steps executed on the CPU. For a computational setup featuring a $12$-core AMD Opteron 6344 CPU clocked at $2.60$ GHz alongside an NVIDIA Tesla K20Xm GPU, Mohammadi et al. report a runtime of $0.47$ seconds for ACLM when applied to the \textit{in-2004} graph with $13.6$ million edges (refer to Table 6 in \cite{com-mohammadi20}). Moreover, they document runtimes of $3.71$, $0.60$, and $0.95$ seconds for PLM \cite{staudt2015engineering}, APLM \cite{com-fazlali17}, and Naim et al. \cite{com-naim17} respectively (also presented in Table 6 of \cite{com-mohammadi20}). In contrast, leveraging our system equipped with two $16$-core Intel Xeon Gold 6226R CPUs operating at $2.90$ GHz (delivering up to $3.0\times$ higher performance), we process the \textit{indochina-2004} graph boasting $341$ million edges ($25.1\times$ larger) in merely $0.65$ seconds. Consequently, our Louvain implementation outperforms ACLM, PLM, APLM, and Naim et al. by approximately $6.0\times$, $48\times$, $7.7\times$, and $12.2\times$ respectively, all achieved without GPU utilization.
% Mohammadi et al. \cite{com-mohammadi20} propose the Adaptive CUDA Louvain Method (ACLM), which uses GPU to calculate the modularity sigma in parallel. For each edge in the graph, they compute the change in modularity of moving the source vertex to the community of the target vertex. Only the computation of change in modularity are performed on the GPU --- remaining steps of the algorithm are still performed on the CPU.\ignore{They allocate threads to the block adaptively to calculate modularity, and make dynamic usage of shared memory in each block.} On a system with an $12$ core AMD Opteron 6344 CPU running at $2.60$ GHz and an NVIDIA Tesla K20Xm GPU, and on the \textit{in-2004} graph with $13.6M$ edges, they observe a runtime of $0.47$ seconds for ACLM (see Table 6 of their paper \cite{com-mohammadi20}). Further, they observe runtimes of $3.71$, $0.60$, and $0.95$ seconds for PLM \cite{staudt2015engineering}, APLM \cite{com-fazlali17}, and Naim et al. \cite{com-naim17} respectively (also Table 6 of their paper \cite{com-mohammadi20}). In contrast, on our system with two $16$ core Intel Xeon Gold 6226R CPUs running at $2.90$ GHz (upto $3.0\times$ faster), we process the \textit{indochina-2004} graph with $341M$ edges ($25.1\times$ larger) in $0.65$ seconds. Thus, our Louvain implementation is roughly $6.0\times$, $48\times$, $7.7\times$, and $12.2\times$ faster than ACLM, PLM, APLM, and Naim et al., without using a GPU.

Sattar and Arifuzzaman \cite{sattar2022scalable} discuss their Distributed Parallel Louvain Algorithm with Load-balancing (DPLAL) implementation. They first partition the input graph using METIS, with the edge-cut partitioning approach. In each pass, for each graph, they compute the best community to move to, change communities while addressing duality issues, compute the the new modularity, and generate the next level graph. This process repeats until there is no further increase in modularity. However, they only perform one iteration of local-moving phase per pass, which may lead to poor-quality communities being identified.\ignore{It is not clear if they perform local-moves to the best community per vertex, or just any community with a positive delta-modularity score. They do not present the quality of returned communities.} On a graph with $1M$ vertices, and while using $40$ compute node of the Louisiana Optical Network Infrastructure (LONI) QB2 compute cluster, where each compute node has two $10$ core $2.80$ GHz Intel Xeon E5-2680v2 CPUs, they obtain communities in roughly $80$ seconds (see Figure 4 of their paper \cite{sattar2022scalable}). In contrast, on our system with two $16$ core Intel Xeon Gold 6226R CPUs running at $2.90$ GHz ($6.0\times$ slower, assuming DPLAL utilizes each compute node with only $25\%$ efficiency), we process the \textit{indochina-2004} graph with $7.41M$ vertex ($7.41\times$ larger) in $0.65$ seconds. Thus, our Louvain implementation is roughly $5472\times$ faster than DPLAL.

Qie et al. \cite{qie2022isolate} discuss a graph partitioning algorithm that divides the graph into partition sets in which vertices are relatively decoupled from others, similar to graph coloring approach proposed by Halappanavar et al. \cite{com-halappanavar17} - minimizing communication delay and avoiding community swaps. We do not observe community swap issue on CPUs (it likely resolves itself), but do observe it on GPUs (likely due to lockstep execution). On a system with an $8$ core Intel i9 9900K CPU running at $3.60$ GHz, and on the \textit{com-LiveJournal} graph, they observe a runtime of about $1050$ seconds (see Figure 3 of their paper \cite{qie2022isolate}). In contrast, on our system with two $16$ core Intel Xeon Gold 6226R CPUs running at $2.90$ GHz (about $3.2\times$ faster), we process the same graph in $1.2$ seconds. Thus, our Louvain implementation is roughly $273\times$ faster.

Bhowmick et al. \cite{com-bhowmik19} propose HyDetect, a divide-and-conquer community detection algorithm for hybrid CPU-GPU systems. The graph representing a network is partitioned among the CPU and GPU devices of a node, and independent community detection using Louvain algorithm is carried out in both the parts. On a system with a six core Intel Xeon E5-2620 running at $2.00$ GHz and an NVIDIA Kepler K40M GPU, the observe a runtime of $1123$ seconds on the \textit{it-2004} graph (see Table 5 in their paper \cite{com-bhowmik19}). In contrast, on our system with two $16$ core Intel Xeon Gold 6226R CPUs running at $2.90$ GHz (about $7.7\times$ faster), we process the same graph in $2.7$ seconds. Thus, our Louvain implementation is around $54\times$ faster than HyDetect, without using a GPU.

Bhowmick et al. \cite{com-bhowmick22} also present a multi-node multi-GPU Louvain community detection algorithm. They partition the graph across multiple nodes, refine by identification of doubtful vertices and migrating them to the other processors, and use a hierarchical merging algorithm that ensures that at any point the merged component can be accommodated within a processor. They use a CrayXC40 system, where each node is equipped with one $12$ core $2.40$ GHz Intel Xeon Ivybridge E5-2695 v2 CPU and one Nvidia Tesla K40 GPU. They are able find communities on the \textit{uk-2005} graph on $8$ compute nodes in about $32$ seconds (see Figure 8 of their paper \cite{com-bhowmick22}). Further, they observe speedup of about $1.7\times$ relative to the work of Cheong et al. \cite{com-cheong13} on the \textit{uk-2005} graph (see Figure 11 of their paper \cite{com-bhowmick22}), and a speedup of about $1.2\times$ with respect to the work of Ghost et al. \cite{com-ghosh18} on the \textit{sk-2005} graph (see Figure 10 of their paper \cite{com-bhowmick22}). In contrast, on our system with two $16$ core Intel Xeon Gold 6226R CPUs running at $2.90$ GHz (upto $1.6\times$ faster, assuming Bhowmick et al. utilize each compute node with only $25\%$ efficiency), we process the \textit{uk-2005} graph in $4.1$ seconds. Thus, our Louvain implementation is at least $4.9\times$, $8.3\times$, and $5.9\times$ faster than Multi-node HyDetect \cite{com-bhowmick22}, Cheong et al. \cite{com-cheong13}, and Ghost et al. \cite{com-ghosh18}, without using a single GPU.

Chou and Ghosh \cite{chou2022batched} discuss a batched method for clustering on GPUs that allow users to achieve the desirable amount of quality without compromising the performance. Due to a partition-based design, they can process graphs larger than the combined GPU memory of a node. On a system with two $128$ core AMD EPYC 7742 CPUs running at $2.25$ GHz and eight NVIDIA A100 GPUs, they observe a (geometric) mean speedup of $2.4\times$ with respect to Grappolo \cite{com-halappanavar17}, using $128$ threads (see Table 3 in their paper \cite{chou2022batched}). Further, they observe that cuGraph \cite{hricik2020using} offers a mean speedup of $7.7\times$ with respect to Grappolo (also Table 3 in their paper \cite{chou2022batched}). In contrast, we obtain a mean speedup of $22\times$ relative to Grappolo. Thus, our Louvain implementation is $9.2\times$ faster than Chou and Ghosh, and $2.9\times$ faster than cuGraph, without using a single GPU.

Gawande et al. \cite{com-gawande22} present cuVite, their ongoing work on distributed Louvain for heterogeneous systems. They build on their prior work parallelizing the Louvain method for community detection on traditional CPU-only distributed systems without GPUs. They evaluate cuVite, cuGraph, and Rundemanen on a single-GPU of NVIDIA DGX-2, i.e., NVIDIA Tesla V100, and with two $24$ core Intel Xeon Platinum 8168 CPUs running at $2.7$ GHz. Further, they evaluate Grappolo a 224 core system with eight $28$ core Intel Xeon Platinum 8276M CPUs running at $2.20$ GHz. They observe that, on average, cuGraph, Rundemanen, and cuVite are $3.3\times$, $3.0\times$, and $3.28\times$ faster than Grappolo.\ignore{On the \textit{webbase-2001} graph, cuVite (32 nodes) is $1.3\times$ (incl. remapping) / 2.0x faster (excl. remapping) faster than Grappolo (128 threads), while Vite (32 nodes) is 2.3x faster  (Note, cuVite is not much faster than Vite, in some cases it is slower).} Thus, our Louvain implementation is $6.7\times$ faster than cuGraph and cuVite, and $7.3\times$ faster than Rundemanen, without using a GPU.
