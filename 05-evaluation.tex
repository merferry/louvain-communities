\subsection{Experimental Setup}
\label{sec:setup}

\subsubsection{System used}

We use a server that has two $16$-core x86-based Intel Xeon Gold 6226R processors running at $2.90$ GHz. Each core has an L1 cache of $1$ MB, an L2 cache of $16$ MB, and a shared L3 cache of $22$ MB. The machine has $93.4$ GB of system memory and runs on CentOS Stream 8. For our GPU experiments, we employ a system featuring an NVIDIA A100 GPU (108 SMs, 64 CUDA cores per SM, 80 GB global memory, 1935 GB/s bandwidth, 164 KB shared memory per SM) paired with an AMD EPYC-7742 processor (64 cores, 2.25 GHz). This server is equipped with 512 GB of DDR4 RAM and operates on Ubuntu 20.04.


\subsubsection{Configuration}

We employ 32-bit integers for vertex IDs and 32-bit floats for edge weights, but use 64-bit floats for both computations and hashtable values. Our implementation leverages $64$ threads to align with the number of cores on the system, unless otherwise specified. For compilation, we use GCC 8.5 and OpenMP 4.5 on the CPU system, while on the GPU system, we use GCC 9.4, OpenMP 5.0, and CUDA 11.4.


\subsubsection{Dataset}

The graphs utilized in our experiments are detailed in Table \ref{tab:dataset}. These graphs are sourced from the SuiteSparse Matrix Collection \cite{suite19}. The graphs have vertex counts ranging from $3.07$ to $214$ million and edge counts ranging from $25.4$ million to $3.80$ billion. We ensure that the edges are undirected and weighted, with a default weight of $1$.

\input{src/tab-dataset}
\input{src/fig-louvain-compare}




\subsection{Comparing Performance of GVE-Louvain}
\label{sec:comparison}

We now compare the performance of GVE-Louvain with Vite (Louvain) \cite{ghosh2018scalable}, Grappolo (Louvain) \cite{com-halappanavar17}, NetworKit Louvain \cite{staudt2016networkit}, and cuGraph Louvain \cite{kang2023cugraph}. For Vite, we convert the graph datasets to Vite's binary graph format, run it on a single node\ignore{(Vite supports distributed community detection)} with threshold cycling/scaling optimization, and measure the reported average total time. For Grappolo, we measure the run it on the same system, and measure the reported total time. For NetworKit Louvain, we use a Python script to invoke \texttt{PLM} (Parallel Louvain Method), and measure the total time reported with \texttt{getTiming()}. To test cuGraph's Leiden algorithm, we write a Python script that configures the Rapids Memory Manager (RMM) to use a pool allocator for fast memory allocations. We then execute \texttt{cugraph.leiden()} on the loaded graph. For each graph, we measure the runtime and the modularity of the obtained communities (as reported by each implementation), performing five runs to calculate an average. When using cuGraph, we discard the runtime of the first run to ensure that subsequent measurements accurately reflect RMM's pool usage without the overhead of initial CUDA memory allocation.

Figure \ref{fig:louvain-compare--runtime} shows the runtimes of Vite (Louvain), Grappolo (Louvain), NetworKit Louvain, cuGraph Louvain, and GVE-Louvain on each graph in the dataset. cuGraph's Leiden algorithm fails to run on the \textit{arabic-2005}, \textit{uk-2005}, \textit{webbase-2001}, \textit{it-2004}, and \textit{sk-2005} graphs because of out-of-memory issues. On the \textit{sk-2005} graph, GVE-Louvain finds communities in $6.8$ seconds, and thus achieve a processing rate of $560$ million edges/s. Figure \ref{fig:louvain-compare--speedup} shows the speedup of GVE-Louvain with respect to each implementation mentioned above. GVE-Louvain is on average $50\times$, $22\times$, $20\times$, and $5.8\times$ faster than Vite, Grappolo, NetworKit Louvain, and cuGraph Louvain respectively.  Figure \ref{fig:louvain-compare--modularity} shows the modularity of communities obtained with each implementation. GVE-Louvain on average obtains $3.1\%$ higher modularity than Vite (especially on web graphs), and $0.6\%$ lower modularity than Grappolo and NetworKit (especially on social networks with poor clustering), and $2.6\%$ higher modularity than cuGraph Louvain (primarily because cuGraph Leiden failed to run on graphs that are well-clusterable).

\input{src/fig-louvain-splits}
\input{src/fig-louvain-hardness}
\input{src/fig-louvain-ss}




\subsection{Analyzing Performance of GVE-Louvain}

The phase-wise and pass-wise split of GVE-Louvain is shown in Figures \ref{fig:louvain-splits--phase} and \ref{fig:louvain-splits--pass} respectively. Figure \ref{fig:louvain-splits--phase} indicates that GVE-Louvain spends most of the runtime in the local-moving phase on \textit{web graphs}, \textit{road networks}, and \textit{protein k-mer graphs}, while it devotes majority of the runtime in the aggregation phase on \textit{social networks}. The pass-wise split (Figure \ref{fig:louvain-splits--pass}) indicates that the first pass dominates runtime on high-degree graphs (\textit{web graphs} and \textit{social networks}), while subsequent passes prevail in execution time on low-degree graphs (\textit{road networks} and \textit{protein k-mer graphs}).

On average, $49\%$ of GVE-Louvain's runtime is spent in the local-moving phase, $35\%$ is spent in the aggregation phase, and $16\%$ is spent in other steps (initialization, renumbering communities, looking up dendrogram, and resetting communities) of the algorithm. Further, $67\%$ of the runtime is spent in the first pass of the algorithm, which is the most expensive pass due to the size of the original graph (later passes work on super-vertex graphs) \cite{com-wickramaarachchi14}.

We also observe that graphs with lower average degree (\textit{road networks} and \textit{protein k-mer graphs}) and graphs with poor community structure (such as \verb|com-LiveJournal| and \verb|com-Orkut|) have a larger $\text{runtime}/|E|$ factor, as shown in Figure \ref{fig:louvain-hardness}.





\subsection{Strong Scaling of GVE-Louvain}

Finally, we measure the strong scaling performance of GVE-Louvain. To this end, we adjust the number of threads from $1$ to $64$ in multiples of $2$ for each input graph, and measure the overall time taken for finding communities with GVE-Louvain, as well as its phase splits (local-moving, aggregation, others), five times for averaging. The results are shown in Figure \ref{fig:louvain-ss}. With 32 threads, GVE-Louvain obtains an average speedup of $10.4\times$ compared to running with a single thread, i.e., its performance increases by $1.6\times$ for every doubling of threads. Scaling is limited due to the various sequential steps/phases in the algorithm. At 64 threads, GVE-Louvain is impacted by NUMA effects, and offers speedup of only $11.4\times$.
